{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10efb981",
   "metadata": {},
   "source": [
    "## Using closing price to analyze returns\n",
    "\n",
    "We will get the -Close- columns to analyze returns. Adjusted Close prices include dividends and splits so we can clearly visualize accurate returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "\n",
    "tickers = [\"IVV\", \"IEF\", \"GLD\"] \n",
    "path = '../data/sample_data.csv'\n",
    "data = pd.read_csv(path, index_col=0, parse_dates=True, header=[0, 1])\n",
    "price = pd.DataFrame({t: data[t]['Close'] for t in tickers})\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(price.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcab41",
   "metadata": {},
   "source": [
    "### Visualizing returns\n",
    "\n",
    "Before calculating returns, let's visualize price levels to get a sense of how these assets behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the price evolution\n",
    "\n",
    "price.plot(figsize=(10,5), title=\"Price Evolution of IVV, IEF & GLD\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price (USD)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b6d97",
   "metadata": {},
   "source": [
    "This graph represents the raw price process $P_t$.\n",
    "\n",
    "Let's interpret what this graph is telling us about each of the assets.\n",
    "\n",
    "Â· IVV: It shows long-term growth and strong cycles each couple of years. The price has increased greatly in the last 5 years.\n",
    "\n",
    "Â· IEF: The growth is quite slow and smooth. This also an indicative of lower volatility.\n",
    "\n",
    "Â· GLD: Gold is often named a safe-haven asset. It is resistant to crises and inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b707749",
   "metadata": {},
   "source": [
    "We have an ordered sequence of daily closing prices for each ETF we have chosen on \"tickers\". We will be analyzing stationarity and volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9263fe",
   "metadata": {},
   "source": [
    "### Calculating daily returns\n",
    "\n",
    "We will use the formula $r_t = P_t / P_{t-1}-1 = \\frac{P_t -P_{t-1}}{P_{t-1}}.$ This is interpretable as \"the price rose by x% compared to the previous day\". To calculate accumulated wealth: $(1+r_1)(1+r_2)\\cdots -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily returns \n",
    "\n",
    "simple_returns = price.pct_change().dropna()\n",
    "print(\"First 5 rows:\")\n",
    "print(simple_returns.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b23268",
   "metadata": {},
   "source": [
    "### Calculating Logarithmic returns\n",
    "These are often used for mathematical/statistical analysis. The formula is:\n",
    "\n",
    "$ l_t = ln(\\dfrac{P_t}{P_{t-1}}) $\n",
    "\n",
    "Log-returns are additive in time, which means the sum of log-returns equals the log of composed return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic returns \n",
    "\n",
    "log_returns = np.log(price / price.shift(1)).dropna()  # ln(P_t / P_{t-1})\n",
    "print(\"Logarithmic returns calculated successfully.\")\n",
    "print(\"First 5 rows:\")\n",
    "print(log_returns.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7abd7",
   "metadata": {},
   "source": [
    "### Cumulative return\n",
    "\n",
    "We will check the accumulated value of an investment of $1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbad6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# By columns, evolution of $1 -> cumulative value\n",
    "\n",
    "cum_simple = (1 + simple_returns).cumprod()  \n",
    "print(cum_simple.head())\n",
    "\n",
    "# Another way; index of normalized price (P_t / P_0)\n",
    "\n",
    "cum_price_index = price / price.iloc[0]      \n",
    "print(\"\\nNormalized price index calculated successfully.\")\n",
    "print(cum_price_index.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ab928",
   "metadata": {},
   "source": [
    "Another way of calculating it is using the mathematical property. We will plot the results to observe the growth factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bda525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative log returns \n",
    "\n",
    "cumulative_returns = np.exp(log_returns.cumsum())\n",
    "\n",
    "cumulative_returns.plot(figsize=(10,5), title=\"Cumulative Log Returns (Growth of $1)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Growth Factor\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151097e3",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The curves indicate how an investment of $1 in 2005 would have evolved. The shapes of the curves represent compounding effects and volatility drag. \n",
    "\n",
    "Comparing the curves, it is shown that in the long run, IVV and GLD performed about the same, showing a similar behaviour in the last decade.\n",
    "\n",
    "IEF, on the contrary, performed a steady growth, several times smaller than the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad74413",
   "metadata": {},
   "source": [
    "### Correlation and Covariance\n",
    "\n",
    "Now we will study the underlying diversification potential and correlations between the log returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation and covariance \n",
    "\n",
    "corr = log_returns.corr()\n",
    "cov = log_returns.cov()\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "print(corr)\n",
    "\n",
    "print(\"\\nCovariance matrix:\")\n",
    "print(cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-analysis-intro",
   "metadata": {},
   "source": [
    "## ðŸ“Š Enhanced Analysis: Rolling Statistics and Risk Metrics\n",
    "\n",
    "Now we'll add three key enhancements to deepen our understanding before moving to full statistical analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolling-stats-explanation",
   "metadata": {},
   "source": [
    "### ðŸ”„ 1. Rolling Statistics\n",
    "\n",
    "Financial time series often exhibit **time-varying properties**. Rolling statistics help us visualize:\n",
    "- How volatility changes over time (volatility clustering)\n",
    "- How correlations evolve (important for diversification)\n",
    "- Market regimes and structural breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolling-statistics-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ENHANCEMENT 1: Rolling Statistics\n",
    "# ===========================================================================\n",
    "\n",
    "# Import seaborn for better visualizations\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# 1.1 Calculate rolling volatility (standard deviation)\n",
    "# Using 21-day window (approximately 1 trading month) and 252 days (trading year)\n",
    "window_short = 21  # Short-term: 1 month\n",
    "window_long = 252  # Long-term: 1 year\n",
    "\n",
    "# Calculate rolling annualized volatility\n",
    "rolling_vol_short = log_returns.rolling(window=window_short).std() * np.sqrt(252)\n",
    "rolling_vol_long = log_returns.rolling(window=window_long).std() * np.sqrt(252)\n",
    "\n",
    "# Plot rolling volatility\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot short-term rolling volatility\n",
    "for ticker in tickers:\n",
    "    axes[0].plot(rolling_vol_short.index, rolling_vol_short[ticker], \n",
    "                label=ticker, linewidth=1.5, alpha=0.8)\n",
    "axes[0].set_title(f'{window_short}-Day Rolling Volatility (Annualized)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Volatility')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot long-term rolling volatility\n",
    "for ticker in tickers:\n",
    "    axes[1].plot(rolling_vol_long.index, rolling_vol_long[ticker], \n",
    "                label=ticker, linewidth=1.5, alpha=0.8)\n",
    "axes[1].set_title(f'{window_long}-Day Rolling Volatility (Annualized)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Volatility')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 1.2 Calculate rolling correlation between IVV and IEF (stock-bond correlation)\n",
    "# This is crucial for understanding diversification benefits over time\n",
    "rolling_corr_window = 63  # Approximately 3 months\n",
    "rolling_corr = log_returns['IVV'].rolling(window=rolling_corr_window).corr(log_returns['IEF'])\n",
    "\n",
    "# Plot rolling correlation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rolling_corr.index, rolling_corr, 'b-', linewidth=2, alpha=0.8)\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Zero Correlation')\n",
    "plt.axhline(y=rolling_corr.mean(), color='g', linestyle='--', alpha=0.5, \n",
    "           label=f'Mean: {rolling_corr.mean():.3f}')\n",
    "plt.fill_between(rolling_corr.index, rolling_corr, 0, \n",
    "                where=(rolling_corr < 0), color='red', alpha=0.1, label='Negative Correlation')\n",
    "plt.fill_between(rolling_corr.index, rolling_corr, 0, \n",
    "                where=(rolling_corr >= 0), color='green', alpha=0.1, label='Positive Correlation')\n",
    "plt.title(f'{rolling_corr_window}-Day Rolling Correlation: IVV vs IEF', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for rolling correlation\n",
    "print(\"ROLLING CORRELATION SUMMARY (IVV vs IEF):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Window: {rolling_corr_window} days\")\n",
    "print(f\"Mean correlation: {rolling_corr.mean():.4f}\")\n",
    "print(f\"Std deviation: {rolling_corr.std():.4f}\")\n",
    "print(f\"Minimum: {rolling_corr.min():.4f}\")\n",
    "print(f\"Maximum: {rolling_corr.max():.4f}\")\n",
    "print(f\"% Negative days: {(rolling_corr < 0).sum() / len(rolling_corr.dropna()) * 100:.1f}%\")\n",
    "print(f\"% Positive days: {(rolling_corr >= 0).sum() / len(rolling_corr.dropna()) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distribution-analysis-explanation",
   "metadata": {},
   "source": [
    "### ðŸ“‰ 2. Distribution Analysis\n",
    "\n",
    "Understanding the **shape of return distributions** is critical for risk management:\n",
    "- **Skewness**: Measures asymmetry (negative = more extreme losses)\n",
    "- **Kurtosis**: Measures tail thickness (high = more extreme events)\n",
    "- **Jarque-Bera test**: Tests if returns follow normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distribution-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ENHANCEMENT 2: Distribution Analysis\n",
    "# ===========================================================================\n",
    "\n",
    "# Import statistical functions\n",
    "from scipy import stats\n",
    "from scipy.stats import jarque_bera, skew, kurtosis\n",
    "\n",
    "# Create comprehensive distribution analysis\n",
    "distribution_results = []\n",
    "\n",
    "print(\"DISTRIBUTION ANALYSIS OF ETF RETURNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for ticker in tickers:\n",
    "    returns = log_returns[ticker].dropna()\n",
    "    \n",
    "    # Calculate distribution statistics\n",
    "    mean_return = returns.mean()\n",
    "    std_return = returns.std()\n",
    "    skewness = skew(returns)\n",
    "    kurt = kurtosis(returns)  # Excess kurtosis (normal = 0)\n",
    "    \n",
    "    # Jarque-Bera test for normality\n",
    "    jb_stat, jb_pval = jarque_bera(returns)\n",
    "    \n",
    "    # Store results\n",
    "    distribution_results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Mean': mean_return,\n",
    "        'Std': std_return,\n",
    "        'Skewness': skewness,\n",
    "        'Kurtosis': kurt,\n",
    "        'JB_Statistic': jb_stat,\n",
    "        'JB_p-value': jb_pval,\n",
    "        'Is_Normal': jb_pval > 0.05  # Fail to reject null hypothesis at 5% level\n",
    "    })\n",
    "    \n",
    "    # Print individual results\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Mean return: {mean_return:.6f}\")\n",
    "    print(f\"  Std deviation: {std_return:.6f}\")\n",
    "    print(f\"  Skewness: {skewness:.4f} ({'Negative' if skewness < 0 else 'Positive'} skew)\")\n",
    "    print(f\"  Kurtosis: {kurt:.4f} ({'Fat tails' if kurt > 0 else 'Thin tails'})\")\n",
    "    print(f\"  Jarque-Bera test: Stat={jb_stat:.2f}, p-value={jb_pval:.6f}\")\n",
    "    print(f\"  Normal distribution? {'YES' if jb_pval > 0.05 else 'NO'}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "dist_df = pd.DataFrame(distribution_results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DISTRIBUTION SUMMARY TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(dist_df.to_string(index=False))\n",
    "\n",
    "# Visualize distributions with histograms and KDE\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    returns = log_returns[ticker].dropna()\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    sns.histplot(returns, kde=True, ax=axes[idx, 0], bins=100, \n",
    "                color=f'C{idx}', edgecolor='black', alpha=0.6)\n",
    "    \n",
    "    # Add normal distribution overlay for comparison\n",
    "    x = np.linspace(returns.min(), returns.max(), 1000)\n",
    "    normal_pdf = stats.norm.pdf(x, returns.mean(), returns.std())\n",
    "    axes[idx, 0].plot(x, normal_pdf, 'r-', linewidth=2, alpha=0.7, label='Normal Distribution')\n",
    "    \n",
    "    axes[idx, 0].set_title(f'{ticker} Return Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx, 0].set_xlabel('Daily Log Return')\n",
    "    axes[idx, 0].set_ylabel('Frequency')\n",
    "    axes[idx, 0].legend()\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add distribution statistics as text box\n",
    "    stats_text = f\"Mean: {returns.mean():.6f}\\n\"\n",
    "    stats_text += f\"Std: {returns.std():.6f}\\n\"\n",
    "    stats_text += f\"Skew: {skew(returns):.4f}\\n\"\n",
    "    stats_text += f\"Kurtosis: {kurtosis(returns):.4f}\\n\"\n",
    "    stats_text += f\"JB p-value: {jarque_bera(returns)[1]:.6f}\"\n",
    "    axes[idx, 0].text(0.02, 0.98, stats_text, transform=axes[idx, 0].transAxes,\n",
    "                     fontsize=9, verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # QQ Plot for normality check\n",
    "    stats.probplot(returns, dist=\"norm\", plot=axes[idx, 1])\n",
    "    axes[idx, 1].set_title(f'{ticker} QQ Plot', fontsize=12, fontweight='bold')\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation of distribution results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION OF DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. SKEWNESS:\")\n",
    "print(\"   - Negative skewness indicates more frequent extreme negative returns\")\n",
    "print(\"   - Positive skewness indicates more frequent extreme positive returns\")\n",
    "print(\"   - Ideal: Slightly positive or zero skew\")\n",
    "\n",
    "print(\"\\n2. KURTOSIS (Excess):\")\n",
    "print(\"   - > 0: Fat tails (more extreme events than normal distribution)\")\n",
    "print(\"   - < 0: Thin tails (fewer extreme events)\")\n",
    "print(\"   - Normal distribution has excess kurtosis = 0\")\n",
    "\n",
    "print(\"\\n3. JARQUE-BERA TEST:\")\n",
    "print(\"   - Null hypothesis: Returns are normally distributed\")\n",
    "print(\"   - p-value > 0.05: Fail to reject null (normal distribution)\")\n",
    "print(\"   - p-value <= 0.05: Reject null (NOT normal distribution)\")\n",
    "\n",
    "print(\"\\n4. IMPLICATIONS:\")\n",
    "print(\"   - Non-normal distributions affect risk measures like VaR\")\n",
    "print(\"   - Fat tails mean extreme events occur more frequently than expected\")\n",
    "print(\"   - Negative skew means downside risk is greater than upside potential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "risk-metrics-explanation",
   "metadata": {},
   "source": [
    "### âš ï¸ 3. Risk Metrics\n",
    "\n",
    "**Sharpe Ratio** is the most widely used risk-adjusted performance measure:\n",
    "$$\\text{Sharpe Ratio} = \\frac{E[R] - R_f}{\\sigma}$$\n",
    "Where:\n",
    "- $E[R]$ = Expected return\n",
    "- $R_f$ = Risk-free rate\n",
    "- $\\sigma$ = Standard deviation of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "risk-metrics-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ENHANCEMENT 3: Risk Metrics (Sharpe Ratio)\n",
    "# ===========================================================================\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.02, periods=252):\n",
    "    \"\"\"\n",
    "    Calculate annualized Sharpe ratio for a return series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Daily return series\n",
    "    risk_free_rate : float\n",
    "        Annual risk-free rate (default 2%)\n",
    "    periods : int\n",
    "        Number of trading periods in a year (default 252)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sharpe_ratio : float\n",
    "        Annualized Sharpe ratio\n",
    "    \"\"\"\n",
    "    # Calculate excess returns (returns above risk-free rate)\n",
    "    excess_returns = returns - risk_free_rate/periods\n",
    "    \n",
    "    # Annualize mean and standard deviation\n",
    "    annualized_mean = excess_returns.mean() * periods\n",
    "    annualized_std = returns.std() * np.sqrt(periods)\n",
    "    \n",
    "    # Calculate Sharpe ratio\n",
    "    if annualized_std > 0:\n",
    "        sharpe_ratio = annualized_mean / annualized_std\n",
    "    else:\n",
    "        sharpe_ratio = np.nan\n",
    "    \n",
    "    return sharpe_ratio\n",
    "\n",
    "# Calculate Sharpe ratios for each ETF\n",
    "sharpe_results = {}\n",
    "\n",
    "print(\"SHARPE RATIO ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Assumptions:\\n- Risk-free rate: 2% annual\\n- Trading days per year: 252\\n\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    sharpe = calculate_sharpe_ratio(log_returns[ticker])\n",
    "    sharpe_results[ticker] = sharpe\n",
    "    \n",
    "    # Interpret Sharpe ratio\n",
    "    if sharpe > 1.0:\n",
    "        interpretation = \"Excellent (> 1.0)\"\n",
    "    elif sharpe > 0.5:\n",
    "        interpretation = \"Good (0.5 - 1.0)\"\n",
    "    elif sharpe > 0:\n",
    "        interpretation = \"Acceptable (0 - 0.5)\"\n",
    "    elif sharpe > -0.5:\n",
    "        interpretation = \"Poor (-0.5 - 0)\"\n",
    "    else:\n",
    "        interpretation = \"Very Poor (< -0.5)\"\n",
    "    \n",
    "    print(f\"{ticker}:\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe:.3f}\")\n",
    "    print(f\"  Interpretation: {interpretation}\")\n",
    "\n",
    "# Compare Sharpe ratios visually\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart of Sharpe ratios\n",
    "tickers_list = list(sharpe_results.keys())\n",
    "sharpe_values = list(sharpe_results.values())\n",
    "\n",
    "bars = axes[0].bar(tickers_list, sharpe_values, color=['#2E86AB', '#A23B72', '#F18F01'])\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "axes[0].axhline(y=1, color='green', linestyle='--', linewidth=1, alpha=0.7, label='Excellent (1.0)')\n",
    "axes[0].axhline(y=0.5, color='orange', linestyle='--', linewidth=1, alpha=0.7, label='Good (0.5)')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.7, label='Zero')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "axes[0].set_title('Annualized Sharpe Ratios', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Sharpe Ratio')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Risk-Return scatter plot\n",
    "for ticker in tickers:\n",
    "    annual_return = log_returns[ticker].mean() * 252\n",
    "    annual_volatility = log_returns[ticker].std() * np.sqrt(252)\n",
    "    sharpe = sharpe_results[ticker]\n",
    "    \n",
    "    axes[1].scatter(annual_volatility, annual_return, s=200, alpha=0.7, label=ticker)\n",
    "    axes[1].text(annual_volatility + 0.002, annual_return + 0.002, \n",
    "                f'{ticker}\\nSharpe: {sharpe:.2f}', \n",
    "                fontsize=10, ha='left', va='bottom')\n",
    "\n",
    "# Add capital market line (theoretical optimal)\n",
    "x_range = np.linspace(0, axes[1].get_xlim()[1], 100)\n",
    "risk_free_rate = 0.02\n",
    "max_sharpe = max(sharpe_results.values())\n",
    "cml = risk_free_rate + max_sharpe * x_range\n",
    "axes[1].plot(x_range, cml, 'r--', alpha=0.5, label=f'Capital Market Line\\n(Max Sharpe: {max_sharpe:.2f})')\n",
    "\n",
    "axes[1].set_xlabel('Annual Volatility (Risk)')\n",
    "axes[1].set_ylabel('Annual Return')\n",
    "axes[1].set_title('Risk-Return Trade-off', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional risk metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ADDITIONAL RISK METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate and display additional metrics\n",
    "risk_metrics = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    returns = log_returns[ticker].dropna()\n",
    "    \n",
    "    # Basic metrics\n",
    "    annual_return = returns.mean() * 252\n",
    "    annual_volatility = returns.std() * np.sqrt(252)\n",
    "    sharpe = sharpe_results[ticker]\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Value at Risk (95% confidence, historical)\n",
    "    var_95 = np.percentile(returns, 5)  # 5th percentile for 95% confidence\n",
    "    \n",
    "    # Sortino ratio (downside risk only)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    downside_std = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0\n",
    "    sortino = (annual_return - 0.02) / downside_std if downside_std > 0 else np.nan\n",
    "    \n",
    "    risk_metrics.append({\n",
    "        'ETF': ticker,\n",
    "        'Ann. Return': annual_return,\n",
    "        'Ann. Volatility': annual_volatility,\n",
    "        'Sharpe': sharpe,\n",
    "        'Sortino': sortino,\n",
    "        'Max Drawdown': max_drawdown,\n",
    "        'VaR 95%': var_95,\n",
    "        'Return/Vol': annual_return/annual_volatility if annual_volatility > 0 else np.nan\n",
    "    })\n",
    "\n",
    "risk_df = pd.DataFrame(risk_metrics)\n",
    "risk_df.set_index('ETF', inplace=True)\n",
    "\n",
    "print(\"\\nComprehensive Risk Metrics Table:\")\n",
    "print(\"-\" * 80)\n",
    "print(risk_df.round(4))\n",
    "\n",
    "# Identify best and worst performers\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE RANKINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Highest Sharpe Ratio: {risk_df['Sharpe'].idxmax()} ({risk_df['Sharpe'].max():.3f})\")\n",
    "print(f\"2. Lowest Volatility: {risk_df['Ann. Volatility'].idxmin()} ({risk_df['Ann. Volatility'].min():.3f})\")\n",
    "print(f\"3. Highest Return: {risk_df['Ann. Return'].idxmax()} ({risk_df['Ann. Return'].max():.3f})\")\n",
    "print(f\"4. Best Risk-Adjusted Return (Return/Vol): {risk_df['Return/Vol'].idxmax()} ({risk_df['Return/Vol'].max():.3f})\")\n",
    "print(f\"5. Worst Maximum Drawdown: {risk_df['Max Drawdown'].idxmin()} ({risk_df['Max Drawdown'].min():.3f})\")\n",
    "\n",
    "# Save results for next analysis\n",
    "risk_df.to_csv('../data/risk_metrics_summary.csv')\n",
    "print(\"\\nâœ“ Risk metrics saved to '../data/risk_metrics_summary.csv'\")\n",
    "print(\"âœ“ Ready for advanced statistical analysis in next notebook! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-enhanced",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary of Enhancements\n",
    "\n",
    "We've successfully added three key enhancements to our EDA:\n",
    "\n",
    "### 1. **Rolling Statistics**\n",
    "- Visualized how volatility changes over time\n",
    "- Analyzed rolling correlation between stocks and bonds\n",
    "- Identified periods of high/low volatility clustering\n",
    "\n",
    "### 2. **Distribution Analysis**\n",
    "- Calculated skewness and kurtosis for each ETF\n",
    "- Performed Jarque-Bera normality tests\n",
    "- Created visual distributions with QQ plots\n",
    "- Found that returns are NOT normally distributed (fat tails)\n",
    "\n",
    "### 3. **Risk Metrics**\n",
    "- Calculated Sharpe ratios for risk-adjusted performance\n",
    "- Added Sortino ratio for downside risk focus\n",
    "- Computed maximum drawdown and Value at Risk\n",
    "- Created risk-return scatter plots\n",
    "\n",
    "### Next Steps:\n",
    "These enhancements provide a solid foundation for the **03_statistical_analysis.ipynb**, which will dive deeper into:\n",
    "- Stationarity tests (ADF, KPSS)\n",
    "- Advanced volatility modeling (GARCH)\n",
    "- Autocorrelation analysis\n",
    "- Comprehensive hypothesis testing\n",
    "\n",
    "The enhanced EDA now gives us a complete picture of our data's behavior before moving to more complex statistical modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
